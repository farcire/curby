# LLM Evaluation Strategy for Parking Regulations

This document defines the strategy for using a "Judge" LLM to evaluate and improve the interpretations generated by the "Worker" LLM.

## 1. The Judge's Rubric

The Judge evaluates interpretations based on a strict hierarchy of error types. This ensures that critical safety issues (like getting towed) are prioritized over minor stylistic issues.

### ðŸ”´ Critical Safety Errors (Score: 0.0 - Must Flag)
**Definition:** Any error that could lead to the user's vehicle being **ticketed or towed**.
*   **False Permission:** The interpretation implies parking is allowed when the text explicitly prohibits it (e.g., "No Parking" interpreted as "Parking Allowed").
*   **Tow-Away Omission:** The text mentions "Tow-Away" or "Tow Away Zone" but the interpretation misses this critical warning.
*   **Time Window Expansion (The "Trap"):** The interpretation suggests a restriction ends *earlier* or starts *later* than it actually does, creating a "false safe zone."
    *   *Example:* Text says "No Parking 7AM - 9AM". AI says "No Parking 7AM - 8AM".
    *   *Result:* User parks at 8:30AM believing it is safe, but gets **Ticketed/Towed**.

### ðŸŸ  Accuracy Errors (Score: 0.5 - Flag for Review)
**Definition:** The interpretation is factually wrong, vague, or incomplete, but **does NOT** create a risk of towing. It mostly causes confusion or lost utility.
*   **Temporal Mismatch (The "Inconvenience"):** The interpretation is over-cautious.
    *   *Example:* Text says "No Parking Mon-Fri". AI says "No Parking Mon-Sat".
    *   *Result:* User avoids parking on Saturday when they actually could have.
*   **Vague Summary:** The summary is too generic (e.g., "Restrictions apply" instead of "No parking for oversized vehicles").
*   **Condition Conflict:** The summary says one thing ("No Parking") but the structured conditions say another ("Action: Allowed").

### ðŸŸ¡ Minor Omissions (Score: 0.8 - Pass with Warning)
**Definition:** Cosmetic issues, formatting quirks, or missing minor exceptions that rarely apply.
*   **Missing Rare Exception:** Misses "Except State Vehicles" or "Except Diplomatic Corps".
*   **Stylistic Issues:** Use of "Do not park" instead of "No Parking".
*   **Icon Mismatch:** Choosing a generic "warning" icon instead of a specific "street cleaning" icon.

### ðŸŸ¢ Perfect Interpretation (Score: 1.0 - Auto-Approve)
*   Accurately captures the rule, time limits, days, and all exceptions.
*   "Action" field correctly maps to the severity of the rule.

## 2. Worker Prompt Strategy

The "Worker" model is responsible for the initial interpretation. To ensure high quality, we use a **Few-Shot Prompting** strategy with diverse examples.

### System Persona
> "You are an expert Parking Data Analyst for the SFMTA. Your goal is to extract structured rules from unstructured regulation text. You prioritize accuracy and clarity. If a rule is ambiguous, you mark it as such."

### Severity Guidelines for Worker Output
The `severity` field indicates the risk of being towed or ticketed:
*   **critical**: Absolute prohibitions where stopping or standing is forbidden. Risk of immediate towing. (e.g., "No Stopping", "Tow Away Zone", "No Parking Anytime").
*   **high**: Strict parking prohibitions. (e.g., "Commercial Loading Only").
*   **medium**: Conditional parking. You can park, but must obey limits. (e.g., "2 Hour Parking", "Metered Parking").
*   **low**: Informational or definitional text. Does not inherently prohibit parking without other signs. (e.g., "Permit Area W Boundaries").

### Few-Shot Examples (Rated by Difficulty)

**Example 1: The "Critical" Standard (Prohibition)**
*   **Complexity:** Low
*   **Key Challenge:** Catching the absolute prohibition.
*   **Input:** "NO STOPPING ANYTIME"
*   **Output:** `{"action": "prohibited", "summary": "No stopping or parking at any time", "severity": "critical"}`
*   **Severity Reasoning:** "NO STOPPING" implies an immediate safety hazard or traffic blockage. The risk is immediate towing.

**Example 2: The "Trap" Avoider (Compound Time Limits)**
*   **Complexity:** High
*   **Key Challenge:** Parsing multiple time windows and days without expanding them (Safety) or shrinking them (Accuracy).
*   **Input:** "2 HR PARKING 9AM-6PM MON-SAT EXCEPT PERMIT W"
*   **Output:** `{"action": "time-limited", "summary": "2 Hour Parking 9AM-6PM Mon-Sat", "conditions": {"time_limit": 120, "days": ["Mon"..."Sat"], "exceptions": ["Permit W"]}, "severity": "medium"}`
*   **Severity Reasoning:** Parking is allowed, but conditional. If you violate the time limit, you get a ticket, but usually not an immediate tow (unless in a commute lane, which would be a separate restriction).

**Example 3: The "Implicit Rule" Handler (Definitions as Restrictions)**
*   **Complexity:** Medium
*   **Key Challenge:** Recognizing that a definition of a vehicle class in the regulations often implies a restriction on that class.
*   **Input:** "Oversized vehicles and trailers are those longer than 22 feet"
*   **Output:** `{"action": "prohibited", "summary": "No parking for oversized vehicles (>22ft)", "details": "Restriction applies to vehicles longer than 22 feet.", "severity": "low"}`
*   **Severity Reasoning:** While framed as a definition, in the context of parking regulations, defining "Oversized Vehicles" functions as a supplemental rule prohibiting them. Therefore, it is treated as a high-severity restriction for that specific vehicle class.
    *   *Correction/Context:* This is classified as **low severity** because most drivers will not have vehicles > 22ft. It applies to a small subset of users (RVs, mobile homes) and is not the primary use case for the average driver. The intent is to prevent indefinite habitation on streets.

## 3. The Feedback Loop

The system improves over time through a continuous "Human-in-the-Loop" workflow.

### Workflow Steps
1.  **Batch Processing:** The Worker interprets a batch of regulations.
2.  **Judge Evaluation:** The Judge scores all interpretations.
    *   **Pass (Score > 0.9):** Auto-committed to the database.
    *   **Fail (Score < 0.9):** Sent to the **Review Queue**.
3.  **Human Review:** A developer reviews the Review Queue.
    *   **Correct:** The human fixes the JSON output. Specifically, fixing the `interpreted` field structure or content.
    *   **Learn:** The corrected example is added to the "Golden Set".
4.  **Prompt Evolution:**
    *   New edge cases from the Golden Set are added as "Few-Shot Examples" in the Worker prompt.
    *   If the Judge consistently mis-flags items, the Judge's rubric is refined.

## 4. Success Metrics (Production Readiness)

We define "Production Ready" based on three key metrics:

| Metric | Target | Description |
| :--- | :--- | :--- |
| **Judge Approval Rate** | **> 90%** | The percentage of interpretations the Judge approves without flagging. |
| **Safety Accuracy** | **98%** | In a manual audit of 50 "Approved" items, 49+ must have ZERO critical safety errors. |
| **Correction Volume** | **< 50** | The number of items requiring manual review per neighborhood update batch. |